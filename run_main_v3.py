import csv
import requests
import requests.adapters
import requests.exceptions
import concurrent.futures
import time
import json
import os
import datetime

# --- ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (USER CONFIG) ---
API_URL = "http://127.0.0.1:6000/predict"
INPUT_FILE = "C3.csv"
OUTPUT_FILE = "Complete.csv"
CACHE_FILE = "temp_progress.json"
MAX_WORKERS = 5
SAVE_INTERVAL = 100
RETRY_DELAY = 10

# --- üîí Constants (‡πÅ‡∏Å‡πâ Issue Line 114) ---
STATUS_NO_IMAGE = "No Image"
STATUS_API_ERROR = "API Error"
STATUS_SUCCESS = "Success"
STATUS_FAILED = "Failed"

# --- üöÄ ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏π‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (Connection Pooling) ---
session = requests.Session()
adapter = requests.adapters.HTTPAdapter(
    pool_connections=MAX_WORKERS,
    pool_maxsize=MAX_WORKERS,
    max_retries=1
)
session.mount('http://', adapter)
session.mount('https://', adapter)

# --- üõ†Ô∏è Utils Functions ---
def load_cache():
    """‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Cache"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
                return {int(k): v for k, v in loaded.items()}
        except (OSError, json.JSONDecodeError):
            return {}
    return {}

def save_cache(data):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Cache"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
    except OSError:
        pass

def format_time(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

def _prepare_csv_row(row, url_columns, idx, results_map):
    """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1 ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á CSV"""
    new_row = row.copy()
    for col in url_columns:
        res = results_map.get(idx, {}).get(col, {"pea_no": "", "status": "", "method": ""})
        new_row[f"{col}_PEA"] = res["pea_no"]
        
        status_text = res["status"]
        if status_text == STATUS_SUCCESS:
            status_text = res["method"]
        
        new_row[f"{col}_Status"] = status_text
    return new_row

def save_output_csv(filename, headers, url_columns, rows_data, results_map):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV"""
    new_headers = headers.copy()
    for col in url_columns:
        new_headers.append(f"{col}_PEA")
        new_headers.append(f"{col}_Status")

    try:
        with open(filename, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=new_headers)
            writer.writeheader()
            for idx, row in enumerate(rows_data):
                new_row = _prepare_csv_row(row, url_columns, idx, results_map)
                writer.writerow(new_row)
    except OSError as e:
        print(f"‚ö†Ô∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

# --- üß† Logic Functions ---
def _parse_api_response(response):
    """‡πÅ‡∏Å‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å API"""
    if response.status_code == 200:
        data = response.json()
        if data.get("status") == "success":
            result_data = data.get("data", {})
            pea_no = result_data.get("serial_number", "")
            read_method = result_data.get("method", "")
            
            if read_method == "barcode":
                method_display = "Barcode"
            elif read_method == "ocr":
                method_display = "OCR"
            else:
                method_display = read_method
                
            return pea_no, STATUS_SUCCESS, method_display
        else:
            msg = data.get("message", "Unknown")
            is_img_err = "download" in msg.lower() or "image" in msg.lower()
            # ‡πÉ‡∏ä‡πâ Constant ‡πÅ‡∏ó‡∏ô String Literal
            status = STATUS_NO_IMAGE if is_img_err else STATUS_FAILED
            return "", status, msg
            
    elif response.status_code in [400, 404, 422]:
        return "", STATUS_NO_IMAGE, f"API {response.status_code}"
    
    return "", STATUS_API_ERROR, f"HTTP {response.status_code}"

def process_url_task(row_index, col_name, url):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å: ‡∏¢‡∏¥‡∏á API ‡πÅ‡∏•‡∏∞ Retry"""
    if not url or not str(url).strip():
        return row_index, col_name, "", STATUS_NO_IMAGE, ""
    
    clean_url = str(url).strip()
    if not clean_url.lower().startswith("http"):
         return row_index, col_name, "", STATUS_NO_IMAGE, "Invalid URL"

    payload = {"url": clean_url}
    
    while True:
        try:
            response = session.post(API_URL, json=payload, timeout=100)
            pea_no, status, method = _parse_api_response(response)
            return row_index, col_name, pea_no, status, method 
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
            print(f"‚ö†Ô∏è [Row {row_index}] Connection Lost. Retrying in {RETRY_DELAY}s... ({e})")
            time.sleep(RETRY_DELAY)
            continue 
        except Exception as e:
            return row_index, col_name, "", STATUS_API_ERROR, str(e)

# --- üß© Sub-Functions for Main (‡πÅ‡∏Å‡πâ Issue Line 167) ---

def _load_input_csv(filename):
    """‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ headers ‡∏Å‡∏±‡∏ö rows"""
    print(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {filename} ...")
    try:
        with open(filename, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            return reader.fieldnames, list(reader)
    except FileNotFoundError:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {filename}")
        return None, None

def _detect_url_columns(headers, rows_data):
    """‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ http/https"""
    print("üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ Link...")
    url_columns = []
    check_limit = min(len(rows_data), 200)

    for col in headers:
        is_url_col = False
        for i in range(check_limit):
            val = str(rows_data[i].get(col, "")).strip().lower()
            if val.startswith("http://") or val.startswith("https://"):
                is_url_col = True
                break
        if is_url_col:
            url_columns.append(col)
    return url_columns

def _prepare_execution_tasks(rows_data, url_columns, results_map):
    """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° List ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥"""
    tasks = []
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° dict ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô key error
    for idx in range(len(rows_data)):
        if idx not in results_map:
            results_map[idx] = {}

    for idx, row in enumerate(rows_data):
        for col in url_columns:
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
            if col in results_map.get(idx, {}):
                continue
            url = row.get(col, "")
            tasks.append((idx, col, url))
    return tasks

def _execute_and_track(tasks, results_map, headers, url_columns, rows_data):
    """‡∏£‡∏±‡∏ô ThreadPool ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• (‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Complexity ‡∏Ç‡∏≠‡∏á Main)"""
    total_tasks = len(tasks)
    completed_in_session = 0
    start_time = time.time()

    print("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_task = {
            executor.submit(process_url_task, r_idx, col, u): (r_idx, col) 
            for r_idx, col, u in tasks
        }
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_task), 1):
            row_idx, col_name, pea_no, status, method = future.result()
            
            if row_idx not in results_map:
                results_map[row_idx] = {}
                
            results_map[row_idx][col_name] = {
                "pea_no": pea_no,
                "status": status,
                "method": method
            }
            
            completed_in_session += 1
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
            elapsed = time.time() - start_time
            avg_time = elapsed / completed_in_session if completed_in_session > 0 else 0.1
            eta = avg_time * (total_tasks - completed_in_session)
            
            # Print Progress
            if i % 10 == 0 or i == total_tasks:
                speed_txt = f"{1/avg_time:.1f}" if avg_time > 0 else "N/A"
                print(f"‚è≥ [{i}/{total_tasks}] Speed: {speed_txt} img/s | ETA: {format_time(eta)} | Last: {status}")

            # Auto Save
            if i % SAVE_INTERVAL == 0:
                save_cache(results_map)
                save_output_csv(OUTPUT_FILE, headers, url_columns, rows_data, results_map)
                print(f"‚úÖ Auto-Saved ({completed_in_session} done)")

    return start_time # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏£‡∏∏‡∏õ

# --- üèÅ Main Entry Point ---
def main():
    print("="*60)
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° (Auto-Detect Link Columns)")
    print("="*60)
    
    # 1. Load Data
    headers, rows_data = _load_input_csv(INPUT_FILE)
    if not headers:
        return

    # 2. Detect Columns
    url_columns = _detect_url_columns(headers, rows_data)
    if not url_columns:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ Link (http/https) ‡πÄ‡∏•‡∏¢")
        return
    print(f"‚úÖ ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Link ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(url_columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {url_columns}")
    
    # 3. Load Cache & Prepare Tasks
    results_map = load_cache()
    tasks = _prepare_execution_tasks(rows_data, url_columns, results_map)
    
    total_tasks = len(tasks)
    print(f"üìå ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥: {total_tasks}")
    
    if total_tasks == 0:
        print("üéâ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥ (‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß)")
        return

    # 4. Execute Tasks (Logic ‡∏™‡πà‡∏ß‡∏ô‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß)
    start_time = _execute_and_track(tasks, results_map, headers, url_columns, rows_data)

    # 5. Final Save
    print("\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢...")
    save_output_csv(OUTPUT_FILE, headers, url_columns, rows_data, results_map)
    
    total_time = time.time() - start_time
    print("="*60)
    print("üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print(f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {format_time(total_time)}")
    print("="*60)

if __name__ == "__main__":
    main()