import csv
import requests
import requests.adapters
import requests.exceptions
import concurrent.futures
import time
import json
import os
import datetime

# --- ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (USER CONFIG) ---
API_URL = "http://127.0.0.1:6000/predict"
INPUT_FILE = "C3.csv"
OUTPUT_FILE = "Sheet_C3_Complete.csv"
CACHE_FILE = "temp_progress.json"
MAX_WORKERS = 5
SAVE_INTERVAL = 100
RETRY_DELAY = 10

# --- üöÄ ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏π‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (Connection Pooling) ---
session = requests.Session()
adapter = requests.adapters.HTTPAdapter(
    pool_connections=MAX_WORKERS,
    pool_maxsize=MAX_WORKERS,
    max_retries=1
)
session.mount('http://', adapter)
session.mount('https://', adapter)

# --- üõ†Ô∏è Utils Functions ---
def load_cache():
    """‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Cache ‡πÇ‡∏î‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Exception ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° SonarQube"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
                return {int(k): v for k, v in loaded.items()}
        except (OSError, json.JSONDecodeError):
            # Catch specific exceptions instead of bare 'except:'
            return {}
    return {}

def save_cache(data):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Cache ‡πÇ‡∏î‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Exception ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
    except OSError:
        # Catch specific exception related to file I/O
        pass

def format_time(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

def _prepare_csv_row(row, url_columns, idx, results_map):
    """Helper function: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1 ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á CSV (‡∏•‡∏î Complexity)"""
    new_row = row.copy()
    for col in url_columns:
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å results_map ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default
        res = results_map.get(idx, {}).get(col, {"pea_no": "", "status": "", "method": ""})
        
        new_row[f"{col}_PEA"] = res["pea_no"]
        
        # Logic ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Status
        status_text = res["status"]
        if status_text == "Success":
            status_text = res["method"]
        
        new_row[f"{col}_Status"] = status_text
    return new_row

def save_output_csv(filename, headers, url_columns, rows_data, results_map):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Helper function"""
    new_headers = headers.copy()
    
    for col in url_columns:
        new_headers.append(f"{col}_PEA")
        new_headers.append(f"{col}_Status")

    try:
        with open(filename, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=new_headers)
            writer.writeheader()
            
            for idx, row in enumerate(rows_data):
                new_row = _prepare_csv_row(row, url_columns, idx, results_map)
                writer.writerow(new_row)
                
    except OSError as e:
        print(f"‚ö†Ô∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

# --- üß† Logic Functions ---
def _parse_api_response(response):
    """‡πÅ‡∏Å‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å API (‡∏•‡∏î Cognitive Complexity ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ Nested Conditional)"""
    if response.status_code == 200:
        data = response.json()
        if data.get("status") == "success":
            result_data = data.get("data", {})
            pea_no = result_data.get("serial_number", "")
            read_method = result_data.get("method", "")
            
            # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Nested Conditional (Line 85 Issue)
            if read_method == "barcode":
                method_display = "Barcode"
            elif read_method == "ocr":
                method_display = "OCR"
            else:
                method_display = read_method
                
            return pea_no, "Success", method_display
        else:
            msg = data.get("message", "Unknown")
            is_img_err = "download" in msg.lower() or "image" in msg.lower()
            status = "No Image" if is_img_err else "Failed"
            return "", status, msg
            
    elif response.status_code in [400, 404, 422]:
        return "", "No Image", f"API {response.status_code}"
    
    return "", "API Error", f"HTTP {response.status_code}"

def process_url_task(row_index, col_name, url):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å: ‡∏¢‡∏¥‡∏á API ‡πÅ‡∏•‡∏∞ Retry"""
    if not url or not str(url).strip():
        return row_index, col_name, "", "No Image", ""
    
    clean_url = str(url).strip()
    if not clean_url.lower().startswith("http"):
         return row_index, col_name, "", "No Image", "Invalid URL"

    payload = {"url": clean_url}
    
    while True:
        try:
            response = session.post(API_URL, json=payload, timeout=100)
            pea_no, status, method = _parse_api_response(response)
            return row_index, col_name, pea_no, status, method 
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
            print(f"‚ö†Ô∏è [Row {row_index}] Connection Lost. Retrying in {RETRY_DELAY}s... ({e})")
            time.sleep(RETRY_DELAY)
            continue 
        except Exception as e:
            return row_index, col_name, "", "API Error", str(e)

# --- üîç Helper for Main ---
def _detect_url_columns(headers, rows_data):
    """‡πÅ‡∏¢‡∏Å Logic ‡∏Å‡∏≤‡∏£‡∏´‡∏≤ Column Link ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Complexity ‡∏Ç‡∏≠‡∏á Main"""
    print("üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ Link...")
    url_columns = []
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÅ‡∏Ñ‡πà 200 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Å‡πá‡∏û‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    check_limit = min(len(rows_data), 200) 

    for col in headers:
        is_url_col = False
        for i in range(check_limit):
            val = str(rows_data[i].get(col, "")).strip().lower()
            if val.startswith("http://") or val.startswith("https://"):
                is_url_col = True
                break
        
        if is_url_col:
            url_columns.append(col)
            
    return url_columns

# --- üèÅ Main Entry Point ---
def main():
    print("="*60)
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç f-string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (Line 122 Issue)
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° (Auto-Detect Link Columns)")
    print("="*60)
    
    start_time = time.time()
    
    # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {INPUT_FILE} ...")
    results_map = load_cache()
    
    rows_data = []
    headers = []
    
    try:
        with open(INPUT_FILE, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            headers = reader.fieldnames
            rows_data = list(reader)
    except FileNotFoundError:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {INPUT_FILE}")
        return

    # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ Link (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Helper function)
    url_columns = _detect_url_columns(headers, rows_data)

    if not url_columns:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ Link (http/https) ‡πÄ‡∏•‡∏¢ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡πÑ‡∏ü‡∏•‡πå CSV")
        return

    print(f"‚úÖ ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Link ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(url_columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {url_columns}")
    
    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏á‡∏≤‡∏ô
    tasks = []
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° dict ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤
    for idx in range(len(rows_data)):
        if idx not in results_map:
            results_map[idx] = {}

    for idx, row in enumerate(rows_data):
        for col in url_columns:
            if col in results_map.get(idx, {}):
                continue
            
            url = row.get(col, "")
            tasks.append((idx, col, url))

    total_tasks = len(tasks)
    print(f"üìå ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥: {total_tasks}")
    
    if total_tasks == 0:
        print("üéâ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥ (‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß)")
        return

    # 4. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏±‡∏ô Multi-thread
    completed_in_session = 0
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç f-string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (Line 189 Issue)
    print("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_task = {
            executor.submit(process_url_task, r_idx, col, u): (r_idx, col) 
            for r_idx, col, u in tasks
        }
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_task), 1):
            row_idx, col_name, pea_no, status, method = future.result()
            
            if row_idx not in results_map:
                results_map[row_idx] = {}
                
            results_map[row_idx][col_name] = {
                "pea_no": pea_no,
                "status": status,
                "method": method
            }
            
            completed_in_session += 1
            elapsed = time.time() - start_time
            avg_time = elapsed / completed_in_session if completed_in_session > 0 else 0.1
            eta = avg_time * (total_tasks - completed_in_session)
            
            if i % 10 == 0 or i == total_tasks:
                speed_txt = f"{1/avg_time:.1f}" if avg_time > 0 else "N/A"
                print(f"‚è≥ [{i}/{total_tasks}] Speed: {speed_txt} img/s | ETA: {format_time(eta)} | Last: {status}")

            if i % SAVE_INTERVAL == 0:
                save_cache(results_map)
                save_output_csv(OUTPUT_FILE, headers, url_columns, rows_data, results_map)
                print(f"‚úÖ Auto-Saved ({completed_in_session} done)")

    print("\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢...")
    save_output_csv(OUTPUT_FILE, headers, url_columns, rows_data, results_map)
    
    total_time = time.time() - start_time
    print("="*60)
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç f-string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (Line 226 Issue)
    print("üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print(f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {format_time(total_time)}")
    print("="*60)

if __name__ == "__main__":
    main()