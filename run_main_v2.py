import csv
import requests
import requests.adapters
import requests.exceptions
import concurrent.futures
import time
import json
import os
import datetime

# --- ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (USER CONFIG) ---
API_URL = "http://127.0.0.1:5000/predict"
INPUT_FILE = "C3.csv"
OUTPUT_FILE = "Sheet_C3_Complete.csv"
CACHE_FILE = "temp_progress.json"
MAX_WORKERS = 5           
SAVE_INTERVAL = 100
RETRY_DELAY = 10  # (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) ‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πá‡∏ï‡∏´‡∏•‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà

# --- üöÄ ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏π‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß ---
session = requests.Session()
adapter = requests.adapters.HTTPAdapter(
    pool_connections=MAX_WORKERS,
    pool_maxsize=MAX_WORKERS,
    max_retries=1
)
session.mount('http://', adapter)
session.mount('https://', adapter)

def process_url_task(row_index, col_name, url):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡∏¥‡∏á API ‡∏û‡∏£‡πâ‡∏≠‡∏° Logic Status + ‡∏Å‡∏±‡∏ô‡πÄ‡∏ô‡πá‡∏ï‡∏´‡∏•‡∏∏‡∏î"""
    
    if not url or not str(url).strip():
        return row_index, col_name, "", "No Image", ""
    
    clean_url = str(url).strip()
    if not clean_url.lower().startswith("http"):
         return row_index, col_name, "", "No Image", "Invalid URL"

    payload = {"url": clean_url}
    
    # üî• [‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏¢‡∏¥‡∏á‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ (‡∏Å‡∏±‡∏ô‡πÄ‡∏ô‡πá‡∏ï‡∏´‡∏•‡∏∏‡∏î/Server ‡∏î‡∏±‡∏ö)
    while True:
        try:
            # ‡∏¢‡∏¥‡∏á Request
            response = session.post(API_URL, json=payload, timeout=100)
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡πÑ‡∏°‡πà Error ‡∏£‡∏∞‡∏î‡∏±‡∏ö Network) ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏∏‡∏î Loop ‡πÑ‡∏õ‡πÄ‡∏ä‡πá‡∏Ñ Status Code
            if response.status_code == 200:
                data = response.json()
                if data.get("status") == "success":
                    result_data = data.get("data", {})
                    pea_no = result_data.get("serial_number", "")
                    read_method = result_data.get("method", "")
                    
                    method_display = "Barcode" if read_method == "barcode" else ("OCR" if read_method == "ocr" else read_method)
                    return row_index, col_name, pea_no, "Success", method_display
                else:
                    msg = data.get("message", "Unknown")
                    if "download" in msg.lower() or "image" in msg.lower():
                        return row_index, col_name, "", "No Image", msg
                    return row_index, col_name, "", "Failed", msg

            elif response.status_code in [400, 404, 422]:
                return row_index, col_name, "", "No Image", f"API {response.status_code}"
            elif response.status_code >= 500:
                # 500 = Server Error ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ 
                # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏•‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£‡∏ñ‡πâ‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î Server ‡∏û‡∏±‡∏á
                return row_index, col_name, "", "API Error", f"HTTP {response.status_code}"
            else:
                return row_index, col_name, "", "API Error", f"HTTP {response.status_code}"
                
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
            # üî• ‡∏à‡∏±‡∏ö Error ‡πÄ‡∏ô‡πá‡∏ï‡∏´‡∏•‡∏∏‡∏î ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ï‡πà‡∏≠ Server ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î
            print(f"‚ö†Ô∏è [Row {row_index}] Connection Lost/Timeout. Retrying in {RETRY_DELAY}s... ({e})")
            time.sleep(RETRY_DELAY) # ‡∏´‡∏¢‡∏∏‡∏î‡∏£‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
            continue # ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏° while loop ‡πÉ‡∏´‡∏°‡πà
            
        except Exception as e:
            # Error ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Network (‡πÄ‡∏ä‡πà‡∏ô Code ‡∏û‡∏±‡∏á) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            return row_index, col_name, "", "API Error", str(e)

# --- Utils ---
def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
                return {int(k): v for k, v in loaded.items()}
        except: return {}
    return {}

def save_cache(data):
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
    except: pass

def format_time(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

def save_output_csv(filename, headers, url_columns, rows_data, results_map):
    new_headers = headers.copy()
    
    for col in url_columns:
        new_headers.append(f"{col}_PEA")
        new_headers.append(f"{col}_Status")

    try:
        with open(filename, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=new_headers)
            writer.writeheader()
            
            for idx, row in enumerate(rows_data):
                new_row = row.copy()
                for col in url_columns:
                    res = results_map.get(idx, {}).get(col, {"pea_no": "", "status": "", "method": ""})
                    new_row[f"{col}_PEA"] = res["pea_no"]
                    
                    status_text = res["status"]
                    if status_text == "Success":
                        status_text = res["method"]
                    
                    new_row[f"{col}_Status"] = status_text
                writer.writerow(new_row)
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

# --- Main ---
def main():
    print(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {INPUT_FILE} ...")
    
    rows_data = []
    headers = []
    
    try:
        with open(INPUT_FILE, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            headers = reader.fieldnames
            rows_data = list(reader)
    except FileNotFoundError:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {INPUT_FILE}")
        return

    found_urls_set = set()
    for row in rows_data[:20]:
        for col in headers:
            if row[col] and str(row[col]).lower().startswith("http"):
                found_urls_set.add(col)
    
    url_columns = sorted(list(found_urls_set), key=lambda x: (x.split('_')[-1], x))
    
    print(f"üîç ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏∑‡∏≠‡∏ô): {url_columns}")

    results_map = load_cache()
    if results_map:
        print(f"‚ôªÔ∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° {len(results_map)} ‡πÅ‡∏ñ‡∏ß! ‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏•‡∏¢...")

    tasks = []
    skipped_count = 0
    
    for idx, row in enumerate(rows_data):
        for col in url_columns:
            if idx in results_map and col in results_map[idx]:
                skipped_count += 1
                continue
            url = row.get(col, "")
            tasks.append((idx, col, url))

    total_tasks = len(tasks)
    print(f"‚ö° ‡∏á‡∏≤‡∏ô‡πÄ‡∏´‡∏•‡∏∑‡∏≠: {total_tasks} ‡∏£‡∏π‡∏õ (‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß {skipped_count}) | Workers: {MAX_WORKERS}")
    print("-" * 60)

    start_time = time.time()
    completed_in_session = 0
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_task = {executor.submit(process_url_task, r, c, u): (r, c) for (r, c, u) in tasks}
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_task), 1):
            row_idx, col_name, pea_no, status, method = future.result()
            
            if row_idx not in results_map: results_map[row_idx] = {}
            results_map[row_idx][col_name] = {
                "pea_no": pea_no,
                "status": status,
                "method": method
            }
            
            completed_in_session += 1
            
            elapsed = time.time() - start_time
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ 0 ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å
            if completed_in_session > 0:
                avg_time = elapsed / completed_in_session
            else:
                avg_time = 0.1
                
            eta = avg_time * (total_tasks - completed_in_session)
            
            if i % 10 == 0 or i == total_tasks:
                speed_txt = f"{1/avg_time:.1f}" if avg_time > 0 else "N/A"
                print(f"‚è≥ [{i}/{total_tasks}] Speed: {speed_txt} img/s | ETA: {format_time(eta)} | Last: {status}")

            if i % SAVE_INTERVAL == 0:
                save_cache(results_map)
                save_output_csv(OUTPUT_FILE, headers, url_columns, rows_data, results_map)
                print(f"‚úÖ Auto-Saved ({completed_in_session} done)")

    print("\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢...")
    save_output_csv(OUTPUT_FILE, headers, url_columns, rows_data, results_map)

    if os.path.exists(CACHE_FILE): os.remove(CACHE_FILE)
    
    total_time = time.time() - start_time
    print("="*60)
    print(f"üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print(f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {format_time(total_time)}")
    print("="*60)

if __name__ == "__main__":
    main()